import streamlit as st
import os
from pathlib import Path
from typing import List

# LangChain ve Gemini KÃ¼tÃ¼phaneleri (Streamlit ile uyumlu importlar)
from langchain_core.prompts import PromptTemplate 
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA 
from langchain_community.vectorstores import Chroma 
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- Sabitler ve Ayarlar ---
LLM_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004" 
DB_PATH = "rag_store"
DOCS_PATH = "data_docs" 

# API AnahtarÄ±nÄ± sadece ortam deÄŸiÅŸkeninden Ã§eker (Streamlit'in Secrets hatasÄ±nÄ± atlamak iÃ§in)
API_KEY = os.getenv("GEMINI_API_KEY") 

# --- RAG Zinciri Kurulumu ---
@st.cache_resource
def load_rag_chain():
    """RAG zincirini, LLM'i yÃ¼kler ve veritabanÄ±nÄ± kontrol/oluÅŸturur."""

    api_key = API_KEY

    if not api_key:
        st.error("âŒ HATA: GEMINI_API_KEY bulunamadÄ±. LÃ¼tfen Terminal'de export komutuyla ayarlayÄ±n.")
        return None, None

    # 1. Embedding Fonksiyonu
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=api_key)

    # 2. VeritabanÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et ve oluÅŸtur (YalnÄ±zca yoksa)
    if not Path(DB_PATH).exists():
        try:
            # DokÃ¼manlarÄ± yÃ¼kleme
            loader = DirectoryLoader(
                DOCS_PATH, glob="**/*.txt", loader_kwargs={'encoding': 'utf-8', 'errors': 'ignore'}
            )
            docs = loader.load()

            # ParÃ§alara ayÄ±rma (Chunking)
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_documents(docs)

            if not chunks:
                st.error("HATA: Otomatik indeksleme baÅŸarÄ±sÄ±z oldu. DokÃ¼manlar boÅŸ veya okunamÄ±yor.")
                return None, None

            vector_store = Chroma.from_documents(
                documents=chunks, embedding=embeddings, persist_directory=DB_PATH
            )
            st.success("âœ… VeritabanÄ± ilk Ã§alÄ±ÅŸtÄ±rmada baÅŸarÄ±yla oluÅŸturuldu!")

        except Exception as e:
            st.error(f"FATAL HATA: Otomatik indeksleme sÄ±rasÄ±nda beklenmeyen hata oluÅŸtu: {e}")
            return None, None

    else:
        # VeritabanÄ± varsa, sadece yÃ¼kle
        vector_store = Chroma(
            persist_directory=DB_PATH, embedding_function=embeddings
        )

    # 3. RAG Zincirini Kurma
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2, google_api_key=api_key)
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})

    prompt_template = """
    Sen bir Biyomedikal Bilgi AsistanÄ±sÄ±n. AÅŸaÄŸÄ±daki biyomedikal metinleri (Context) kullanarak, kullanÄ±cÄ±ya TÃ¼rkÃ§e ve net bir ÅŸekilde yanÄ±t ver. 
    YanÄ±tlarÄ±n teknik, kÄ±sa ve direkt olmalÄ±dÄ±r. BaÄŸlamda bulamadÄ±ÄŸÄ±n sorulara 'Bu konuda elimde yeterli bilgi yok.' diye yanÄ±t ver.

    Context: {context}
    Soru: {question}
    YanÄ±t:
    """
    PROMPT = PromptTemplate.from_template(prompt_template)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain, retriever 

# RAG zincirini bir kez baÅŸlat
QA_CHAIN, RETRIEVER = load_rag_chain()


# --- Streamlit Ana UygulamasÄ± ---
def main():
    st.set_page_config(page_title="Biyomedikal RAG AsistanÄ±", layout="wide")

    # BaÅŸlÄ±k ve Etik UyarÄ±
    st.markdown(
        """
        <div style="text-align: center; background-color: #1F618D; padding: 15px; border-radius: 10px; color: white;">            <h1>ğŸ”¬ Biyomedikal RAG Bilgi AsistanÄ±</h1>
            <p>Gemini AI, LangChain ve ChromaDB ile gÃ¼Ã§lendirilmiÅŸtir.</p>
        </div>
        """, 
        unsafe_allow_html=True
    )
    st.markdown("---")
    st.warning("ğŸš¨ ETÄ°K UYARI: Bu sistem tÄ±bbi tanÄ±, tedavi veya kiÅŸisel saÄŸlÄ±k tavsiyesi VERMEZ. Sadece bilgi asistanÄ±dÄ±r.")

    # RAG sistemini baÅŸlat
    if not QA_CHAIN:
        st.stop() # Hata varsa durdur

    # Chat MesajlarÄ±
    if 'messages' not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Merhaba! DokÃ¼manlarÄ±mdaki konularla (Ä°mmÃ¼noloji, Etik, Cihazlar) ilgili sorular sorun."}]

    # ArayÃ¼zÃ¼ iki sÃ¼tuna ayÄ±rma
    col1, col2 = st.columns([3, 1])

    with col1: # Ana Sohbet SÃ¼tunu
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # KullanÄ±cÄ±dan Girdi Alma
        if prompt := st.chat_input("Biyomedikal sorunuzu buraya yazÄ±n..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.spinner("ğŸ§  Gemini yanÄ±t oluÅŸturuyor..."):
                # RAG zincirini Ã§alÄ±ÅŸtÄ±rma
                try:
                    result = QA_CHAIN({"query": prompt})
                    response = result['result']
                    docs = result['source_documents']

                    # KaynaklarÄ± ve cevabÄ± birleÅŸtir
                    sources_list = "\n".join([f"- **{d.metadata.get('source', 'Bilinmeyen')}**" for d in docs])

                    full_response = response + "\n\n**Ã‡ekilen Kaynaklar:**\n" + sources_list

                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    with st.chat_message("assistant"):
                        st.markdown(full_response)

                except Exception as e:
                    st.error(f"YanÄ±t oluÅŸturulamadÄ±. Hata: {e}")
                    st.session_state.messages.append({"role": "assistant", "content": "ÃœzgÃ¼nÃ¼m, bir sorun oluÅŸtu."})

    with col2: # Kaynak ve Detay SÃ¼tunu
        st.subheader("Ä°puÃ§larÄ± ve Kaynaklar")
        st.info("Bu model, sadece veri setinde bulunan 14 adet biyomedikal dokÃ¼mandan bilgi Ã§eker.")
        st.markdown("**Ã–rnek Sorular:**")
        st.markdown("- Kalbin en gÃ¼Ã§lÃ¼ odacÄ±ÄŸÄ± nedir?")
        st.markdown("- TÄ±bbi cihazlarÄ±n sÄ±nÄ±flandÄ±rÄ±lmasÄ± nasÄ±l yapÄ±lÄ±r?")
        st.markdown("- Genetik mÃ¼hendisliÄŸinde CRISPR nedir?")
        st.markdown("- Aksiyon potansiyelini baÅŸlatan temel fiziksel mekanizma nedir?")
        st.markdown("- Biyomedikal araÅŸtÄ±rmalarda etik kurallardan biri olan Ã–zerklik ne anlama gelir?")


if __name__ == "__main__":
    main()