import streamlit as st
import os
from pathlib import Path

# LangChain 0.2+/0.3+ uyumlu importlar
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# --- Ayarlar ---
LLM_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004"
DB_PATH = "rag_store"
DOCS_PATH = "data_docs"
API_KEY = os.getenv("AIzaSyBCfEQDr7V9jdDtJlFMIzanlHleejSe1WY")


def _join_docs(docs):
    """Retriever'dan gelen dokÃ¼manlarÄ± tek metne Ã§evirir."""
    return "\n\n".join(d.page_content for d in docs)


@st.cache_resource
def load_rag_chain():
    if not API_KEY:
        st.error("âŒ HATA: GEMINI_API_KEY bulunamadÄ±. LÃ¼tfen ortam deÄŸiÅŸkeni olarak ayarlayÄ±n.")
        return None, None

    # 1) Embedding
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=API_KEY)

    # 2) Chroma DB
    if not Path(DB_PATH).exists():
        try:
            loader = DirectoryLoader(
                DOCS_PATH, glob="**/*.txt", loader_kwargs={"encoding": "utf-8", "errors": "ignore"}
            )
            docs = loader.load()
            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = splitter.split_documents(docs)
            if not chunks:
                st.error("HATA: Ä°ndeksleme baÅŸarÄ±sÄ±z. DokÃ¼manlar boÅŸ veya okunamÄ±yor.")
                return None, None
            vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=DB_PATH)
        except Exception as e:
            st.error(f"FATAL HATA: Otomatik indeksleme sÄ±rasÄ±nda hata: {e}")
            return None, None
    else:
        vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

    # 3) LLM ve Retriever
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2, google_api_key=API_KEY)
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})

    # 4) Prompt (context + input)
    template = """
Sen bir Biyomedikal Bilgi AsistanÄ±sÄ±n. AÅŸaÄŸÄ±daki biyomedikal metinleri (Context) kullanarak, kullanÄ±cÄ±ya TÃ¼rkÃ§e ve net bir ÅŸekilde yanÄ±t ver.
YanÄ±tlarÄ±n teknik, kÄ±sa ve direkt olmalÄ±dÄ±r. BaÄŸlamda bulamadÄ±ÄŸÄ±n sorulara 'Bu konuda elimde yeterli bilgi yok.' diye yanÄ±t ver.

Context:
{context}

Soru:
{input}

YanÄ±t:
"""
    PROMPT = PromptTemplate.from_template(template)

    # 5) LCEL zinciri:
    # retriever -> dokÃ¼manlarÄ± getir -> tek metne Ã§evir
    context_chain = retriever | RunnableLambda(_join_docs)

    # {context, input} -> prompt -> llm -> dÃ¼z metin
    qa_chain = (
        RunnableParallel({"context": context_chain, "input": RunnablePassthrough()})
        | PROMPT
        | llm
        | StrOutputParser()
    )

    return qa_chain, retriever


QA_CHAIN, RETRIEVER = load_rag_chain()


def main():
    st.set_page_config(page_title="Biyomedikal RAG AsistanÄ±", layout="wide")

    st.markdown(
        """
        <div style="text-align: center; background-color: #1F618D; padding: 15px; border-radius: 10px; color: white;">
            <h1>ğŸ”¬ Biyomedikal RAG Bilgi AsistanÄ±</h1>
            <p>Gemini AI, LangChain ve ChromaDB ile gÃ¼Ã§lendirilmiÅŸtir.</p>
        </div>
        """,
        unsafe_allow_html=True,
    )
    st.markdown("---")
    st.warning("ğŸš¨ ETÄ°K UYARI: Bu sistem tÄ±bbi tanÄ±, tedavi veya kiÅŸisel saÄŸlÄ±k tavsiyesi VERMEZ. Sadece bilgi asistanÄ±dÄ±r.")

    if not QA_CHAIN:
        st.stop()

    if "messages" not in st.session_state:
        st.session_state.messages = [{
            "role": "assistant",
            "content": "Merhaba! DokÃ¼manlarÄ±mdaki konularla (Ä°mmÃ¼noloji, Etik, Cihazlar) ilgili sorular sorun."
        }]

    col1, col2 = st.columns([3, 1])

    with col1:
        for m in st.session_state.messages:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

        if prompt := st.chat_input("Biyomedikal sorunuzu buraya yazÄ±n..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.spinner("ğŸ§  Gemini yanÄ±t oluÅŸturuyor..."):
                try:
                    answer_text = QA_CHAIN.invoke(prompt)
                    # Kaynak listesi (pipeline Ã§Ä±ktÄ±sÄ±nda yok; retriever'dan ayrÄ±ca Ã§ekiyoruz)
                    docs = RETRIEVER.get_relevant_documents(prompt)
                    sources_list = "\n".join([f"- **{d.metadata.get('source', 'Bilinmeyen')}**" for d in docs])

                    full_response = answer_text + ("\n\n**Ã‡ekilen Kaynaklar:**\n" + sources_list if sources_list else "")
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    with st.chat_message("assistant"):
                        st.markdown(full_response)
                except Exception as e:
                    st.error(f"YanÄ±t oluÅŸturulamadÄ±. Hata: {e}")
                    st.session_state.messages.append({"role": "assistant", "content": "ÃœzgÃ¼nÃ¼m, bir sorun oluÅŸtu."})

    with col2:
        st.subheader("Ä°puÃ§larÄ± ve Kaynaklar")
        st.info("Bu model, sadece sizin yÃ¼klediÄŸiniz biyomedikal dokÃ¼manlardan bilgi Ã§eker.")
        st.markdown("**Ã–rnek Sorular:**")
        st.markdown("- Kalbin en gÃ¼Ã§lÃ¼ odacÄ±ÄŸÄ± nedir?")
        st.markdown("- TÄ±bbi cihazlarÄ±n sÄ±nÄ±flandÄ±rÄ±lmasÄ± nasÄ±l yapÄ±lÄ±r?")
        st.markdown("- Genetik mÃ¼hendisliÄŸinde CRISPR nedir?")


if __name__ == "__main__":
    main()