import streamlit as st
import os
from pathlib import Path
from typing import List

# --- LangChain 0.2+ uyumlu importlar ---
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter  # DÄ°KKAT: paket adÄ± deÄŸiÅŸti

# --- Sabitler ve Ayarlar ---
LLM_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004"
DB_PATH = "rag_store"
DOCS_PATH = "data_docs"

# API AnahtarÄ±nÄ± ortam deÄŸiÅŸkeninden al
API_KEY = os.getenv("GEMINI_API_KEY")


# --- RAG Zinciri Kurulumu ---
@st.cache_resource
def load_rag_chain():
    """RAG zincirini, LLM'i yÃ¼kler ve veritabanÄ±nÄ± kontrol/oluÅŸturur."""
    api_key = API_KEY

    if not api_key:
        st.error("âŒ HATA: GEMINI_API_KEY bulunamadÄ±. LÃ¼tfen ortam deÄŸiÅŸkeni olarak ayarlayÄ±n.")
        return None, None

    # 1) Embedding
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=api_key)

    # 2) VektÃ¶r veritabanÄ± (oluÅŸtur veya yÃ¼kle)
    if not Path(DB_PATH).exists():
        try:
            loader = DirectoryLoader(
                DOCS_PATH, glob="**/*.txt", loader_kwargs={'encoding': 'utf-8', 'errors': 'ignore'}
            )
            docs = loader.load()

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_documents(docs)

            if not chunks:
                st.error("HATA: Otomatik indeksleme baÅŸarÄ±sÄ±z oldu. DokÃ¼manlar boÅŸ veya okunamÄ±yor.")
                return None, None

            vector_store = Chroma.from_documents(
                documents=chunks, embedding=embeddings, persist_directory=DB_PATH
            )
        except Exception as e:
            st.error(f"FATAL HATA: Otomatik indeksleme sÄ±rasÄ±nda beklenmeyen hata oluÅŸtu: {e}")
            return None, None
    else:
        vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

    # 3) LLM ve Retriever
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2, google_api_key=api_key)
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})

    # 4) Prompt
    # create_stuff_documents_chain iÃ§in deÄŸiÅŸkenler: {context} ve {input}
    prompt_template = """
Sen bir Biyomedikal Bilgi AsistanÄ±sÄ±n. AÅŸaÄŸÄ±daki biyomedikal metinleri (Context) kullanarak, kullanÄ±cÄ±ya TÃ¼rkÃ§e ve net bir ÅŸekilde yanÄ±t ver.
YanÄ±tlarÄ±n teknik, kÄ±sa ve direkt olmalÄ±dÄ±r. BaÄŸlamda bulamadÄ±ÄŸÄ±n sorulara 'Bu konuda elimde yeterli bilgi yok.' diye yanÄ±t ver.

Context:
{context}

Soru:
{input}

YanÄ±t:
"""
    PROMPT = PromptTemplate.from_template(prompt_template)

    # 5) DokÃ¼manlarÄ± "stuff" edip cevaplayan zincir
    doc_chain = create_stuff_documents_chain(llm=llm, prompt=PROMPT)

    # 6) Retriever + doc_chain birleÅŸimi (RetrievalQA karÅŸÄ±lÄ±ÄŸÄ±)
    qa_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=doc_chain)

    return qa_chain, retriever


# RAG zincirini bir kez baÅŸlat
QA_CHAIN, RETRIEVER = load_rag_chain()


# --- Streamlit Ana UygulamasÄ± ---
def main():
    st.set_page_config(page_title="Biyomedikal RAG AsistanÄ±", layout="wide")

    # BaÅŸlÄ±k ve Etik UyarÄ±
    st.markdown(
        """
        <div style="text-align: center; background-color: #1F618D; padding: 15px; border-radius: 10px; color: white;">
            <h1>ğŸ”¬ Biyomedikal RAG Bilgi AsistanÄ±</h1>
            <p>Gemini AI, LangChain ve ChromaDB ile gÃ¼Ã§lendirilmiÅŸtir.</p>
        </div>
        """,
        unsafe_allow_html=True
    )
    st.markdown("---")
    st.warning("ğŸš¨ ETÄ°K UYARI: Bu sistem tÄ±bbi tanÄ±, tedavi veya kiÅŸisel saÄŸlÄ±k tavsiyesi VERMEZ. Sadece bilgi asistanÄ±dÄ±r.")

    if not QA_CHAIN:
        st.stop()

    # Sohbet durumu
    if 'messages' not in st.session_state:
        st.session_state.messages = [{
            "role": "assistant",
            "content": "Merhaba! DokÃ¼manlarÄ±mdaki konularla (Ä°mmÃ¼noloji, Etik, Cihazlar) ilgili sorular sorun."
        }]

    col1, col2 = st.columns([3, 1])

    with col1:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("Biyomedikal sorunuzu buraya yazÄ±n..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.spinner("ğŸ§  Gemini yanÄ±t oluÅŸturuyor..."):
                try:
                    # LCEL zincirini Ã§aÄŸÄ±rma
                    result = QA_CHAIN.invoke({"input": prompt})
                    response = result.get("answer", "")
                    docs = result.get("context", [])

                    sources_list = "\n".join([f"- **{d.metadata.get('source', 'Bilinmeyen')}**" for d in docs])
                    full_response = response + ("\n\n**Ã‡ekilen Kaynaklar:**\n" + sources_list if sources_list else "")

                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    with st.chat_message("assistant"):
                        st.markdown(full_response)

                except Exception as e:
                    st.error(f"YanÄ±t oluÅŸturulamadÄ±. Hata: {e}")
                    st.session_state.messages.append(
                        {"role": "assistant", "content": "ÃœzgÃ¼nÃ¼m, bir sorun oluÅŸtu."}
                    )

    with col2:
        st.subheader("Ä°puÃ§larÄ± ve Kaynaklar")
        st.info("Bu model, sadece sizin yÃ¼klediÄŸiniz biyomedikal dokÃ¼manlardan bilgi Ã§eker.")
        st.markdown("**Ã–rnek Sorular:**")
        st.markdown("- Kalbin en gÃ¼Ã§lÃ¼ odacÄ±ÄŸÄ± nedir?")
        st.markdown("- TÄ±bbi cihazlarÄ±n sÄ±nÄ±flandÄ±rÄ±lmasÄ± nasÄ±l yapÄ±lÄ±r?")
        st.markdown("- Genetik mÃ¼hendisliÄŸinde CRISPR nedir?")


if __name__ == "__main__":
    main()