import streamlit as st
import os
from pathlib import Path

# --- LangChain 0.2+/0.3+ uyumlu importlar (helpers kullanmadan) ---
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from langchain.chains.combine_documents import create_stuff_documents_chain

from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter  # paket adÄ± bu

# --- Sabitler ve Ayarlar ---
LLM_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "models/text-embedding-004"
DB_PATH = "rag_store"
DOCS_PATH = "data_docs"

API_KEY = os.getenv("GEMINI_API_KEY")


@st.cache_resource
def load_rag_chain():
    """RAG zinciri, LLM ve vektÃ¶r veritabanÄ±nÄ± hazÄ±rlar."""
    if not API_KEY:
        st.error("âŒ HATA: GEMINI_API_KEY bulunamadÄ±. LÃ¼tfen ortam deÄŸiÅŸkeni olarak ayarlayÄ±n.")
        return None, None

    # 1) Embedding
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=API_KEY)

    # 2) Chroma DB oluÅŸtur/yÃ¼kle
    if not Path(DB_PATH).exists():
        try:
            loader = DirectoryLoader(
                DOCS_PATH, glob="**/*.txt", loader_kwargs={"encoding": "utf-8", "errors": "ignore"}
            )
            docs = loader.load()

            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = splitter.split_documents(docs)
            if not chunks:
                st.error("HATA: Ä°ndeksleme baÅŸarÄ±sÄ±z. DokÃ¼manlar boÅŸ veya okunamÄ±yor.")
                return None, None

            vector_store = Chroma.from_documents(
                documents=chunks, embedding=embeddings, persist_directory=DB_PATH
            )
        except Exception as e:
            st.error(f"FATAL HATA: Otomatik indeksleme sÄ±rasÄ±nda hata: {e}")
            return None, None
    else:
        vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

    # 3) LLM ve Retriever
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.2, google_api_key=API_KEY)
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})

    # 4) Prompt (create_stuff_documents_chain => {context} + {input})
    prompt_template = """
Sen bir Biyomedikal Bilgi AsistanÄ±sÄ±n. AÅŸaÄŸÄ±daki biyomedikal metinleri (Context) kullanarak, kullanÄ±cÄ±ya TÃ¼rkÃ§e ve net bir ÅŸekilde yanÄ±t ver.
YanÄ±tlarÄ±n teknik, kÄ±sa ve direkt olmalÄ±dÄ±r. BaÄŸlamda bulamadÄ±ÄŸÄ±n sorulara 'Bu konuda elimde yeterli bilgi yok.' diye yanÄ±t ver.

Context:
{context}

Soru:
{input}

YanÄ±t:
"""
    PROMPT = PromptTemplate.from_template(prompt_template)

    # 5) DokÃ¼manlarÄ± LLM'e "stuff" eden zincir
    doc_chain = create_stuff_documents_chain(llm=llm, prompt=PROMPT)

    # 6) create_retrieval_chain yerine RunnableParallel + pipe
    #    - retriever girdiyi alÄ±r ve {context} Ã¼retir
    #    - RunnablePassthrough aynÄ± girdiyi {input} olarak geÃ§irir
    qa_chain = RunnableParallel(
        {"context": retriever, "input": RunnablePassthrough()}
    ) | doc_chain

    return qa_chain, retriever


# Zinciri baÅŸlat
QA_CHAIN, RETRIEVER = load_rag_chain()


def main():
    st.set_page_config(page_title="Biyomedikal RAG AsistanÄ±", layout="wide")

    st.markdown(
        """
        <div style="text-align: center; background-color: #1F618D; padding: 15px; border-radius: 10px; color: white;">
            <h1>ğŸ”¬ Biyomedikal RAG Bilgi AsistanÄ±</h1>
            <p>Gemini AI, LangChain ve ChromaDB ile gÃ¼Ã§lendirilmiÅŸtir.</p>
        </div>
        """,
        unsafe_allow_html=True,
    )
    st.markdown("---")
    st.warning("ğŸš¨ ETÄ°K UYARI: Bu sistem tÄ±bbi tanÄ±, tedavi veya kiÅŸisel saÄŸlÄ±k tavsiyesi VERMEZ. Sadece bilgi asistanÄ±dÄ±r.")

    if not QA_CHAIN:
        st.stop()

    if "messages" not in st.session_state:
        st.session_state.messages = [{
            "role": "assistant",
            "content": "Merhaba! DokÃ¼manlarÄ±mdaki konularla (Ä°mmÃ¼noloji, Etik, Cihazlar) ilgili sorular sorun."
        }]

    col1, col2 = st.columns([3, 1])

    with col1:
        for m in st.session_state.messages:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

        if prompt := st.chat_input("Biyomedikal sorunuzu buraya yazÄ±n..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.spinner("ğŸ§  Gemini yanÄ±t oluÅŸturuyor..."):
                try:
                    # Zinciri Ã§aÄŸÄ±r
                    result = QA_CHAIN.invoke(prompt)
                    # create_stuff_documents_chain bazÄ± sÃ¼rÃ¼mlerde "answer", bazÄ±larÄ±nda "output_text" dÃ¶ndÃ¼rÃ¼r
                    response = result.get("answer") or result.get("output_text") or ""
                    # KaynaklarÄ± ayrÄ±ca al (pipeline Ã§Ä±ktÄ±sÄ±nda yok)
                    docs = RETRIEVER.get_relevant_documents(prompt)
                    sources_list = "\n".join([f"- **{d.metadata.get('source', 'Bilinmeyen')}**" for d in docs])

                    full_response = response + ("\n\n**Ã‡ekilen Kaynaklar:**\n" + sources_list if sources_list else "")
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    with st.chat_message("assistant"):
                        st.markdown(full_response)

                except Exception as e:
                    st.error(f"YanÄ±t oluÅŸturulamadÄ±. Hata: {e}")
                    st.session_state.messages.append({"role": "assistant", "content": "ÃœzgÃ¼nÃ¼m, bir sorun oluÅŸtu."})

    with col2:
        st.subheader("Ä°puÃ§larÄ± ve Kaynaklar")
        st.info("Bu model, sadece sizin yÃ¼klediÄŸiniz biyomedikal dokÃ¼manlardan bilgi Ã§eker.")
        st.markdown("**Ã–rnek Sorular:**")
        st.markdown("- Kalbin en gÃ¼Ã§lÃ¼ odacÄ±ÄŸÄ± nedir?")
        st.markdown("- TÄ±bbi cihazlarÄ±n sÄ±nÄ±flandÄ±rÄ±lmasÄ± nasÄ±l yapÄ±lÄ±r?")
        st.markdown("- Genetik mÃ¼hendisliÄŸinde CRISPR nedir?")


if __name__ == "__main__":
    main()